{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a688b25d-0d66-4c13-9a3d-eedb2bd9baa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: nltk in /usr/local/miniconda3/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/miniconda3/lib/python3.8/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/miniconda3/lib/python3.8/site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/miniconda3/lib/python3.8/site-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: tqdm in /usr/local/miniconda3/lib/python3.8/site-packages (from nltk) (4.65.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9e3cb-677b-4914-bd84-6cce8240864b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:20<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:27<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Train Loss: 2.440, Val. Loss: 9.493, BLEU-4: 0.054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:10<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:27<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Train Loss: 1.662, Val. Loss: 10.000, BLEU-4: 0.047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:22<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:27<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Train Loss: 1.318, Val. Loss: 10.190, BLEU-4: 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:09<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:26<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Train Loss: 1.124, Val. Loss: 10.396, BLEU-4: 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:09<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:26<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Train Loss: 1.041, Val. Loss: 10.675, BLEU-4: 0.053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 3125/3125 [45:10<00:00,  1.15it/s]\n",
      "Evaluating: 100%|██████████| 63/63 [00:26<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Train Loss: 0.991, Val. Loss: 10.840, BLEU-4: 0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▊        | 584/3125 [08:24<40:24,  1.05it/s]"
     ]
    }
   ],
   "source": [
    "# 导入所需的库\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "# 定义数据集类，用于加载和处理数据\n",
    "class ConcodeDataset(Dataset):\n",
    "    def __init__(self, data_file):\n",
    "        with open(data_file, 'r') as f:\n",
    "            self.data = [json.loads(line) for line in f]  # 读取并解析JSON数据\n",
    "        self.build_vocab()  # 构建词汇表\n",
    "\n",
    "    def build_vocab(self):\n",
    "        # 初始化输入和输出词汇表，并添加特殊标记\n",
    "        self.input_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2}\n",
    "        self.output_vocab = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2}\n",
    "        input_idx = len(self.input_vocab)\n",
    "        output_idx = len(self.output_vocab)\n",
    "        \n",
    "        # 遍历数据集，构建输入和输出的词汇表\n",
    "        for item in self.data:\n",
    "            for word in item['nl'].split():\n",
    "                if word not in self.input_vocab:\n",
    "                    self.input_vocab[word] = input_idx\n",
    "                    input_idx += 1\n",
    "            for word in item['code'].split():\n",
    "                if word not in self.output_vocab:\n",
    "                    self.output_vocab[word] = output_idx\n",
    "                    output_idx += 1\n",
    "        self.rev_output_vocab = {idx: word for word, idx in self.output_vocab.items()}  # 构建反向词汇表\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # 返回数据集的长度\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引获取输入和输出序列，并转换为ID表示\n",
    "        input_seq = self.data[index]['nl'].split()\n",
    "        output_seq = self.data[index]['code'].split()\n",
    "        input_ids = [self.input_vocab['<SOS>']] + [self.input_vocab[word] for word in input_seq] + [self.input_vocab['<EOS>']]\n",
    "        output_ids = [self.output_vocab['<SOS>']] + [self.output_vocab[word] for word in output_seq] + [self.output_vocab['<EOS>']]\n",
    "        return torch.tensor(input_ids), torch.tensor(output_ids), self.data[index]['nl'], self.data[index]['code']\n",
    "\n",
    "# 定义编码器类\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)  # 嵌入层，将词汇ID转换为向量表示\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, bidirectional=True)  # 双向GRU层\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout层\n",
    "        self.fc = nn.Linear(hid_dim * 2, hid_dim)  # 全连接层，用于将双向GRU的输出维度减少一半\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        embedded = self.dropout(self.embedding(src))  # 将输入序列嵌入并应用Dropout\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_lengths.cpu(), batch_first=True, enforce_sorted=False)  # 打包嵌入序列\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)  # 通过GRU层\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)  # 解包GRU层的输出\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))  # 处理双向GRU的隐藏状态\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.rnn.num_layers, 1, 1)  # 调整隐藏状态的形状\n",
    "        return outputs, hidden\n",
    "\n",
    "# 定义注意力机制类\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hid_dim * 3, hid_dim)  # 注意力权重计算层\n",
    "        self.v = nn.Linear(hid_dim, 1, bias=False)  # 注意力得分计算层\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]  # 获取源序列长度\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # 扩展隐藏状态\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))  # 计算注意力能量\n",
    "        attention = self.v(energy).squeeze(2)  # 计算注意力得分\n",
    "        return F.softmax(attention, dim=1)  # 计算注意力权重\n",
    "\n",
    "# 定义解码器类\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention  # 引入注意力机制\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)  # 嵌入层\n",
    "        self.rnn = nn.GRU(emb_dim + hid_dim * 2, hid_dim, n_layers, dropout=dropout)  # 带注意力的GRU层\n",
    "        self.fc_out = nn.Linear(emb_dim + hid_dim * 3, output_dim)  # 输出层\n",
    "        self.dropout = nn.Dropout(dropout)  # Dropout层\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        input = input.unsqueeze(0)  # 扩展输入序列的维度\n",
    "        embedded = self.dropout(self.embedding(input))  # 嵌入并应用Dropout\n",
    "        a = self.attention(hidden[-1], encoder_outputs).unsqueeze(1)  # 计算注意力权重\n",
    "        weighted = torch.bmm(a, encoder_outputs)  # 计算加权编码器输出\n",
    "        rnn_input = torch.cat((embedded, weighted.transpose(0, 1)), dim=2)  # 拼接嵌入和加权编码器输出\n",
    "        output, hidden = self.rnn(rnn_input, hidden)  # 通过GRU层\n",
    "        hidden = hidden.squeeze(0)\n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(1)\n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim=1))  # 生成预测结果\n",
    "        return prediction, hidden\n",
    "\n",
    "# 定义序列到序列模型类\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, src_lengths, trg=None, teacher_forcing_ratio=0.75, max_len=100):\n",
    "        trg_len = trg.shape[1] if trg is not None else max_len\n",
    "        batch_size = src.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)  # 初始化输出张量\n",
    "        encoder_outputs, hidden = self.encoder(src, src_lengths)  # 编码器处理输入序列\n",
    "        input = trg[:, 0] if trg is not None else torch.tensor([self.decoder.output_dim-2]*batch_size).to(self.device)  # 初始化解码器输入\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)  # 解码器处理输入并生成输出\n",
    "            outputs[:, t] = output\n",
    "            top1 = output.argmax(1)  # 获取当前时间步的预测结果\n",
    "            input = trg[:, t] if trg is not None and random.random() < teacher_forcing_ratio else top1  # 决定是否使用教师强制\n",
    "            if (top1 == self.decoder.output_dim-1).all():  # 如果预测到结束标记，则停止解码\n",
    "                break\n",
    "        return outputs\n",
    "\n",
    "# 训练模型函数\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(iterator, desc=\"Training\")):\n",
    "        src, src_lengths, trg, _, codes = batch\n",
    "        src = src.to(model.device)\n",
    "        src_lengths = src_lengths.to(model.device)\n",
    "        trg = trg.to(model.device)\n",
    "        optimizer.zero_grad()  # 清除梯度\n",
    "        output = model(src, src_lengths, trg)  # 前向传播\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].contiguous().view(-1, output_dim)  # 调整输出张量的形状\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "        loss = criterion(output, trg)  # 计算损失\n",
    "        loss.backward()  # 反向传播计算梯度\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)  # 梯度裁剪，防止梯度爆炸\n",
    "        optimizer.step()  # 更新模型参数\n",
    "        epoch_loss += loss.item()  # 累加损失\n",
    "    return epoch_loss / len(iterator)  # 返回平均损失\n",
    "\n",
    "# 评估模型函数\n",
    "def evaluate(model, iterator, criterion, concode_dataset, epoch):\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    epoch_loss = 0\n",
    "    all_references = []\n",
    "    all_candidates = []\n",
    "    with torch.no_grad():  # 禁用梯度计算\n",
    "        for i, batch in enumerate(tqdm(iterator, desc=\"Evaluating\")):\n",
    "            src, src_lengths, trg, trg_texts, codes_texts = batch\n",
    "            src = src.to(model.device)\n",
    "            trg = trg.to(model.device)\n",
    "            src_lengths = src_lengths.to(model.device)\n",
    "            output = model(src, src_lengths, max_len=trg.shape[1])  # 前向传播\n",
    "            output_dim = output.shape[-1]\n",
    "            output_convert = output[:, 1:].contiguous().view(-1, output_dim)  # 调整输出张量的形状\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "            loss = criterion(output_convert, trg)  # 计算损失\n",
    "            epoch_loss += loss.item()  # 累加损失\n",
    "            predictions = torch.argmax(output, dim=2)  # 获取预测结果\n",
    "            references = [text.split() for text in codes_texts]  # 参考译文\n",
    "            candidates = [[concode_dataset.rev_output_vocab[idx.item()] for idx in pred if idx.item() in concode_dataset.rev_output_vocab and idx.item() not in {0, 1, 2}] for pred in predictions]  # 生成译文\n",
    "            all_references.extend(references)\n",
    "            all_candidates.extend(candidates)\n",
    "    references_filename = f'codebleu/references_{epoch}.txt'\n",
    "    candidates_filename = f'codebleu/hypothesis_{epoch}.txt'\n",
    "    with open(references_filename, 'w') as ref_file:\n",
    "        for ref in all_references:\n",
    "            ref_file.write(' '.join(ref) + '\\n')\n",
    "    with open(candidates_filename, 'w') as cand_file:\n",
    "        for cand in all_candidates:\n",
    "            cand_file.write(' '.join(cand) + '\\n')\n",
    "    smooth_func = SmoothingFunction().method4\n",
    "    all_references = [[ref] for ref in all_references]\n",
    "    bleu4 = corpus_bleu(all_references, all_candidates, smoothing_function=smooth_func)  # 计算BLEU-4分数\n",
    "    return epoch_loss / len(iterator), bleu4  # 返回平均损失和BLEU-4分数\n",
    "\n",
    "# 加载数据函数\n",
    "def load_data(data_file, batch_size, collate_fn):\n",
    "    dataset = ConcodeDataset(data_file)  # 创建数据集对象\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)  # 创建数据加载器\n",
    "\n",
    "# 数据对齐函数，用于处理变长序列\n",
    "def collate_fn(batch):\n",
    "    src, trg, nls, codes = zip(*batch)\n",
    "    src_lengths = torch.tensor([len(s) for s in src], dtype=torch.int64)\n",
    "    src = pad_sequence(src, padding_value=0, batch_first=True)  # 填充输入序列\n",
    "    trg = pad_sequence(trg, padding_value=0, batch_first=True)  # 填充输出序列\n",
    "    return src, src_lengths, trg, nls, codes\n",
    "\n",
    "# 主函数，定义并训练模型\n",
    "def main():\n",
    "    data_file = 'train.json'\n",
    "    concode_dataset = ConcodeDataset(data_file)  # 创建数据集对象\n",
    "    INPUT_DIM = len(concode_dataset.input_vocab) + 1  # 输入词汇表大小\n",
    "    OUTPUT_DIM = len(concode_dataset.output_vocab) + 1  # 输出词汇表大小\n",
    "    ENC_EMB_DIM = 128  # 编码器嵌入维度\n",
    "    DEC_EMB_DIM = 128  # 解码器嵌入维度\n",
    "    HID_DIM = 256  # 隐藏层维度\n",
    "    N_LAYERS = 2  # RNN层数\n",
    "    ENC_DROPOUT = 0.2  # 编码器Dropout率\n",
    "    DEC_DROPOUT = 0.2  # 解码器Dropout率\n",
    "    BATCH_SIZE = 32  # 批量大小\n",
    "    N_EPOCHS = 10  # 训练轮数\n",
    "    CLIP = 1  # 梯度裁剪值\n",
    "    LEARNING_RATE = 0.001  # 学习率\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 选择设备\n",
    "    train_data = load_data('train.json', BATCH_SIZE, collate_fn)  # 加载训练数据\n",
    "    valid_data = load_data('dev.json', BATCH_SIZE, collate_fn)  # 加载验证数据\n",
    "    attention = Attention(HID_DIM).to(device)  # 创建注意力机制对象\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT).to(device)  # 创建编码器对象\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, attention).to(device)  # 创建解码器对象\n",
    "    model = Seq2Seq(enc, dec, device).to(device)  # 创建序列到序列模型对象\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # 创建优化器\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)  # 定义损失函数\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        train_loss = train(model, train_data, optimizer, criterion, CLIP)  # 训练模型\n",
    "        valid_loss, bleu4 = evaluate(model, valid_data, criterion, concode_dataset, epoch)  # 评估模型\n",
    "        print(f'Epoch: {epoch + 1:02}, Train Loss: {train_loss:.3f}, Val. Loss: {valid_loss:.3f}, BLEU-4: {bleu4:.3f}')\n",
    "    torch.save(model.state_dict(), 'codebleu_model/seq2seq_gru_model.pt')  # 保存模型\n",
    "\n",
    "    # 测试模型\n",
    "    test_data = load_data('test.json', BATCH_SIZE, collate_fn)\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data, desc=\"Testing\"):\n",
    "            src, src_lengths, _, nls, _ = batch\n",
    "            src = src.to(device)\n",
    "            src_lengths = src_lengths.to(device)\n",
    "            output = model(src, src_lengths, max_len=100)  # 前向传播\n",
    "            predictions = output.argmax(2).transpose(0, 1)  # 获取预测结果\n",
    "            for pred, nl in zip(predictions.cpu().numpy().tolist(), nls):\n",
    "                result = {\n",
    "                    \"code\": \" \".join([concode_dataset.rev_output_vocab[idx] for idx in pred if idx in concode_dataset.rev_output_vocab and idx != 2]),  # 跳过<EOS>标记\n",
    "                    \"nl\": nl\n",
    "                }\n",
    "                results.append(result)\n",
    "    with open('results_1120213587_周圣威.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)  # 保存测试结果为JSON文件\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edeca36-eaa0-4432-9d50-d0c86877f56f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ecedd4-2c9e-435e-84e7-d3761e7bc4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
